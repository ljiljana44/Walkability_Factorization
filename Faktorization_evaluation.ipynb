{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('notebook')\n",
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install surprise\n",
    "%pip install geopy\n",
    "%pip install pyproj\n",
    "%pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Segments transform </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "folder=\"./\"\n",
    "file_path = folder+\"seg_all.csv\"\n",
    "df = pd.read_csv(file_path, delimiter=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['time_1'].isna()==False]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from geopy.distance import geodesic\n",
    "#from pandarallel import pandarallel\n",
    "import pandas as pd\n",
    "\n",
    "# Inicijalizacija pandarallel (koristi sve dostupne jezgre)\n",
    "#pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# Funkcija za računanje vrednosti po redu\n",
    "def process_row(row):\n",
    "    try:\n",
    "        point1 = (row['lat_1'], row['lon_1'])\n",
    "        point2 = (row['lat_2'], row['lon_2'])\n",
    "\n",
    "        # Parsiranje vremena\n",
    "        if 'Z' in row['time_1']:\n",
    "            time1 = datetime.fromisoformat(row['time_1'].replace(\"Z\", \"+00:00\"))\n",
    "            time2 = datetime.fromisoformat(row['time_2'].replace(\"Z\", \"+00:00\"))\n",
    "        else:\n",
    "            time1 = datetime.fromisoformat(row['time_1'])\n",
    "            time2 = datetime.fromisoformat(row['time_2'])\n",
    "\n",
    "        # Razlika u nadmorskoj visini\n",
    "        elev_diff = row['elev_2'] - row['elev_1']\n",
    "\n",
    "        # Izračunaj rastojanje (u metrima) i vreme (u sekundama)\n",
    "        distance = geodesic(point1, point2).meters\n",
    "        time_diff = (time2 - time1).total_seconds()\n",
    "\n",
    "        # Izračunaj brzinu (m/s) i nagib (%)\n",
    "        speed = distance / time_diff if time_diff > 0 else 0\n",
    "        slope = (elev_diff / distance) * 100 if distance > 0 else 0\n",
    "\n",
    "        return pd.Series([distance, time_diff, speed, slope])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e, row['time_1'])\n",
    "        return pd.Series([None, None, None, None])\n",
    "\n",
    "# Paralelna obrada podataka\n",
    "df[['distance_m', 'time_s', 'speed_m_s', 'slope_percent']] = df.apply(process_row, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Helper function to calculate speed (m/s)\n",
    "def calculate_speed(dist_meters, time_seconds):\n",
    "    return dist_meters / time_seconds if time_seconds > 0 else 0\n",
    "\n",
    "# Helper function to calculate slope\n",
    "def calculate_slope(elev_diff, dist_meters):\n",
    "    return (elev_diff / dist_meters) * 100 if dist_meters > 0 else 0\n",
    "\n",
    "# Add calculated columns\n",
    "distances = []\n",
    "times = []\n",
    "speeds = []\n",
    "slopes = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Get coordinates and timestamps\n",
    "    point1 = (row['lat_1'], row['lon_1'])\n",
    "    point2 = (row['lat_2'], row['lon_2'])\n",
    "    try:\n",
    "      if 'Z' in row['time_1']:\n",
    "\n",
    "        time1 = datetime.fromisoformat(row['time_1'].replace(\"Z\", \"+00:00\"))\n",
    "        time2 = datetime.fromisoformat(row['time_2'].replace(\"Z\", \"+00:00\"))\n",
    "      else :\n",
    "        time1 = datetime.fromisoformat(row['time_1'])\n",
    "        time2 = datetime.fromisoformat(row['time_2'])\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "      print(e,row['time_1'],index)\n",
    "      time1=datetime.fromisoformat(\"2012-10-12\")\n",
    "      time2=datetime.fromisoformat(\"2012-10-12\")\n",
    "\n",
    "\n",
    "    elev_diff = row['elev_2'] - row['elev_1']\n",
    "\n",
    "    # Calculate distance (meters) and time (seconds)\n",
    "    distance = geodesic(point1, point2).meters\n",
    "    time_diff = (time2 - time1).total_seconds()\n",
    "\n",
    "    # Calculate speed (m/s) and slope (%)\n",
    "    speed = calculate_speed(distance, time_diff)\n",
    "    #slope = calculate_slope(elev_diff, distance)\n",
    "\n",
    "    # Append results\n",
    "    distances.append(distance)\n",
    "    times.append(time_diff)\n",
    "    speeds.append(speed)\n",
    "    #slopes.append(slope)\n",
    "\n",
    "# Add results to the DataFrame\n",
    "df['distance_m'] = distances\n",
    "df['time_s'] = times\n",
    "df['speed_m_s'] = speeds\n",
    "#df['slope_percent'] = slopes\n",
    "\n",
    "# Save the results to a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['time_s']>0]\n",
    "df=df[df['time_s']<60]\n",
    "df=df[df['distance_m']<50]\n",
    "df=df[df['distance_m']>0]\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#print(df['time_s'].value_counts())\n",
    "plt.scatter(df['time_s'].value_counts().index,df['time_s'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#print(df['time_s'].value_counts())\n",
    "\n",
    "plt.scatter(df['distance_m'].value_counts().index,df['distance_m'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['speed_m_s']<50]\n",
    "plt.scatter(df['d_from_start'],df['speed_m_s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#print(df['time_s'].value_counts())\n",
    "plt.scatter(df['speed_m_s'].value_counts().index,df['speed_m_s'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['speed_m_s'].describe()\n",
    "\n",
    "#plt.scatter(df['time_s'],df['speed_m_s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['distance_m']<50]\n",
    "df['distance_m'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1=df.groupby('origname')[['speed_m_s']].agg(['mean','median','std']).reset_index()\n",
    "\n",
    "#g1[g1['mean']>15]\n",
    "nevalja=[]\n",
    "for  n in g1[g1['speed_m_s']['median']>20]['origname'] :\n",
    "    print(n)\n",
    "    nevalja.append(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in nevalja:\n",
    "  df=df[df['origname']!=s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Transformer\n",
    "\n",
    "trans = Transformer.from_crs(\n",
    "    \"epsg:4326\",\n",
    "  #  \"+proj=utm +zone=10 +ellps=WGS84\",\n",
    "  \"epsg:3765\",\n",
    "    always_xy=True,\n",
    ")\n",
    "#32610\n",
    "xx, yy = trans.transform(df[\"lon_1\"].values, df[\"lat_1\"].values)\n",
    "xx2, yy2 = trans.transform(df[\"lon_2\"].values, df[\"lat_2\"].values)\n",
    "\n",
    "df[\"N1\"] = xx\n",
    "df[\"E1\"] = yy\n",
    "df[\"N2\"] = xx2\n",
    "df[\"E2\"] = yy2\n",
    "import math\n",
    "frameForGroup=df.copy()\n",
    "num_round=-1\n",
    "def distance_utm(row):\n",
    "  n1=row.lat_1\n",
    "  e1=row.lon_1\n",
    "  n2=row.lat_2\n",
    "  e2=row.lon_2\n",
    "  x=n2-n1\n",
    "  y=e2-e1\n",
    "  c=math.sqrt(x*x+y*y)\n",
    "  return c\n",
    "def calc_slope(e_d,d_D):\n",
    "  if e_d==0:\n",
    "    return 0\n",
    "  if d_D==0:\n",
    "    return 0\n",
    "  return e_d/(d_D*1000)\n",
    "frameForGroup['lat_1']=round(frameForGroup['N1'], num_round)\n",
    "frameForGroup['lon_1']=round(frameForGroup['E1'], num_round)\n",
    "frameForGroup['lat_2']=round(frameForGroup['N2'], num_round)\n",
    "frameForGroup['lon_2']=round(frameForGroup['E2'], num_round)\n",
    "frameForGroup['dist_utm']=frameForGroup.apply(distance_utm,axis=1)\n",
    "#frame['angle_utm']=frame.apply (lambda row: calc_slope(row['elev_d'],row['dist_utm']) , axis=1) # df['elev_d']/(df['dist_2d_wgs']*1000)\n",
    "\n",
    "frameForGroup['speed_utm']=frameForGroup['dist_utm']/frameForGroup['time_s']\n",
    "#frameForGroup['angle_utm']=frameForGroup['elev_d']/frameForGroup['dist_utm']\n",
    "#frameForGroup['angle_utm']=frameForGroup.apply (lambda row: calc_slope(row['elev_d'],row['dist_utm']) , axis=1) # df['elev_d']/(df['dist_2d_wgs']*1000)\n",
    "\n",
    "#frameForGroup=frameForGroup[frameForGroup['time_d_s']<50]\n",
    "frameForGroup=frameForGroup[frameForGroup['speed_utm']<10]\n",
    "#frameForGroup['latlon']=frameForGroup.apply (lambda row: str(row['lat_1']).strip()+'_'+str(row['lon_1']).strip()+'_'+str(row['lat_2']).strip()+'_'+str(row['lon_2']).strip() , axis=1)\n",
    "\n",
    "\n",
    "frameForGroup['latlon']=frameForGroup.apply (lambda row: str(row['lat_1']).strip()+'_'+str(row['lon_1']).strip()  , axis=1)\n",
    "\n",
    "#frameForGroup['latlon']=frameForGroup.apply (lambda row: str(row['lat_1']).strip()+'_'+str(row['lon_1']).strip()+'_'+str(row['lat_2']).strip()+'_'+str(row['lon_2']).strip() , axis=1)\n",
    "frameForGroup[\"origname_encoded\"] = frameForGroup[\"origname\"].astype(\"category\").cat.codes\n",
    "\n",
    "\n",
    "# Normalize frequencies to avoid dominance of this feature in PCA\n",
    "frameForGroup['latlon_encoded'] = frameForGroup[\"latlon\"].astype(\"category\").cat.codes\n",
    "\n",
    "frameForGroup.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameForGroup['latlon'].value_counts()[frameForGroup['latlon'].value_counts()>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_frame=frameForGroup[['lat_1', 'lon_1',   'lat_2', 'lon_2','distance_m', 'time_s', 'speed_m_s',\n",
    "       'slope_percent',    'origname_encoded', 'N1', 'E1', 'N2',\n",
    "       'E2', 'dist_utm','latlon_encoded']].groupby(['latlon_encoded','origname_encoded']).agg('median')\n",
    "dd=grouped_frame[[ 'distance_m', 'time_s', 'speed_m_s', 'slope_percent' ]].apply(lambda x: x)\n",
    "\n",
    "dd=dd.reset_index()\n",
    "\n",
    "col_sel=['elev_1','distance_m', 'time_s', 'speed_m_s',\n",
    "       'slope_percent', 'curvature_rad']\n",
    "\n",
    "col_Sel2=[]\n",
    "numerical_data=dd[['latlon_encoded','origname_encoded','speed_m_s']]\n",
    "numerical_data.to_csv('numerical_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>frameforgroup</h1>\n",
    ">>frameforgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "frameForGroup=pd.read_csv('frameForGroup-2.csv')\n",
    "frameForGroup[['latlon','latlon_encoded']]\n",
    "grouped_frame=frameForGroup[['lat_1', 'lon_1',   'lat_2', 'lon_2','distance_m', 'time_s', 'speed_m_s',\n",
    "        'origname_encoded', 'N1', 'E1', 'N2',\n",
    "       'E2', 'latlon_encoded']].groupby(['latlon_encoded','origname_encoded']).agg('median')\n",
    "dd=grouped_frame[[ 'distance_m', 'time_s', 'speed_m_s' ]].apply(lambda x: x)\n",
    "\n",
    "dd=dd.reset_index()\n",
    "numerical_data=dd[['latlon_encoded','origname_encoded','speed_m_s']]\n",
    "#numerical_data=frameForGroup[['origname_encoded','latlon_encoded','speed_m_s']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical_data['latlon_encoded'][numerical_data['latlon_encoded']==.value_counts()==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameForGroup[['latlon','latlon_encoded']]\n",
    "frameForGroup['latlon_encoded'].unique()\n",
    "latlon_dict = frameForGroup.set_index('latlon_encoded')['latlon'].to_dict()\n",
    "lat_dict=frameForGroup.set_index('latlon_encoded')['lat_1'].to_dict()\n",
    "lon_dict=frameForGroup.set_index('latlon_encoded')['lon_1'].to_dict()\n",
    "N1_dict=frameForGroup.set_index('latlon_encoded')['N1'].to_dict()\n",
    "E1_dict=frameForGroup.set_index('latlon_encoded')['E1'].to_dict()\n",
    "frameForGroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NMF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF,non_negative_factorization\n",
    "from sklearn.metrics import mean_squared_error\n",
    "ratings_dict = {\n",
    "    \"user\": numerical_data['origname_encoded'].values,\n",
    "    \"item\": numerical_data['latlon_encoded'].values,\n",
    "    \"rating\": numerical_data['speed_m_s'].values\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(ratings_dict)\n",
    "user_item_matrix = df.pivot(index=\"user\", columns=\"item\", values=\"rating\").fillna(0)\n",
    "#print(user_item_matrix)\n",
    "\n",
    "nmf = NMF(n_components=1, init='nndsvd', random_state=42)\n",
    "\n",
    "# Factorize user-item matrix into W (Users) and H (Items)\n",
    "W_nmf = nmf.fit_transform(user_item_matrix)  # User features\n",
    "H_nmf = nmf.components_  # Item features\n",
    "\n",
    "#print(\"User Features Matrix (W):\\n\", W)\n",
    "#print(\"Item Features Matrix (H):\\n\", H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix.shape\n",
    "df.shape[0]/user_item_matrix.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>evaluate nmf</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_ratings = np.dot(W_nmf, H_nmf)\n",
    "reconstructed_ratings\n",
    "user_item_matrix[user_item_matrix>0]\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(user_item_matrix, reconstructed_ratings))\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>non_negative_factorization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_non, H_non, n_iter = non_negative_factorization(user_item_matrix, n_components=1, init='random', random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_ratings = np.dot(W_non, H_non)\n",
    "reconstructed_ratings\n",
    "user_item_matrix[user_item_matrix>0]\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(user_item_matrix, reconstructed_ratings))\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SVD s</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# ✅ Example Dataset\n",
    " \n",
    "\n",
    "# Create a sample dataset (user_id, item_id, rating)\n",
    " \n",
    "\n",
    "# ✅ 1. Create User-Item Matrix\n",
    "#user_item_matrix = df_train.pivot(index=\"user_id\", columns=\"item_id\", values=\"rating\").fillna(0)\n",
    "\n",
    "# ✅ 2. Apply SVD (Truncated SVD for matrix factorization)\n",
    "n_factors = 1  # Number of latent factors\n",
    "new_var = user_item_matrix.values\n",
    "U, S, Vt = svds(new_var, k=n_factors)\n",
    "\n",
    "# Convert singular values into a diagonal matrix\n",
    "S_diag = np.diag(S)\n",
    "\n",
    "# ✅ 3. Reconstruct the matrix\n",
    "reconstructed_matrix = np.dot(np.dot(U, S_diag), Vt)\n",
    "\n",
    "# ✅ 4. Function to Predict Ratings\n",
    "def predict_rating(user, item):\n",
    "    \"\"\"\n",
    "    Predict rating for a given user and item.\n",
    "    :param user: user index\n",
    "    :param item: item index\n",
    "    :return: predicted rating\n",
    "    \"\"\"\n",
    "    if user < reconstructed_matrix.shape[0] and item < reconstructed_matrix.shape[1]:\n",
    "        return reconstructed_matrix[user, item]\n",
    "    else:\n",
    "        return np.nan  # Return NaN if out of bounds\n",
    "\n",
    "# Example prediction for user 0 and item 10\n",
    "predicted_rating = predict_rating(0, 10)\n",
    "print(f\"Predicted rating for user 0 and item 10: {predicted_rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>evaluate svd</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_ratings = np.dot(U, Vt)\n",
    "reconstructed_ratings\n",
    "user_item_matrix[user_item_matrix>0]\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(user_item_matrix, reconstructed_ratings))\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TruncatedSVD</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    " \n",
    "\n",
    "svd = TruncatedSVD(n_components=1)\n",
    "user_factors = svd.fit_transform(user_item_matrix)  # User embeddings\n",
    "item_factors = svd.components_.T  # Item embeddings (transposed)\n",
    "\n",
    "print(\"User Factors (Embeddings):\")\n",
    "print(user_factors.shape)\n",
    "\n",
    "print(\"Item Factors (Embeddings):\")\n",
    "print(item_factors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_ratings = np.dot(user_factors, item_factors.T)\n",
    "actual_ratings = user_item_matrix[user_item_matrix > 0]\n",
    "predicted_ratings = reconstructed_ratings[user_item_matrix > 0]\n",
    "rmse = np.sqrt(mean_squared_error(user_item_matrix, reconstructed_ratings))\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reconstructed_ratings.shape,actual_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import fastica\n",
    "K, W, S = fastica(user_item_matrix, n_components=1, random_state=0, whiten='unit-variance')\n",
    "u_factors=S\n",
    "i_factors=K \n",
    "i_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_ratings = np.dot(u_factors, i_factors)\n",
    "actual_ratings = user_item_matrix[user_item_matrix > 0]\n",
    "predicted_ratings = reconstructed_ratings[user_item_matrix > 0]\n",
    "rmse = np.sqrt(mean_squared_error(user_item_matrix, reconstructed_ratings))\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SGD </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class SGDMatrixFactorization:\n",
    "    def __init__(self, num_users, num_items, n_factors=10, learning_rate=0.01, reg_param=0.02, n_epochs=20):\n",
    "        \"\"\"\n",
    "        Initialize the matrix factorization model using SGD.\n",
    "        \"\"\"\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_param = reg_param\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        # Initialize user and item latent factors randomly\n",
    "        self.user_factors = np.random.normal(scale=0.1, size=(num_users, n_factors))\n",
    "        self.item_factors = np.random.normal(scale=0.1, size=(num_items, n_factors))\n",
    "\n",
    "    def train(self, df_train):\n",
    "        \"\"\"\n",
    "        Train the model using stochastic gradient descent (SGD).\n",
    "        :param df_train: Pandas DataFrame with columns ['user_id', 'item_id', 'rating']\n",
    "        \"\"\"\n",
    "        train_data = list(zip(df_train['user'], df_train['item'], df_train['rating']))  # Convert DF to list\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            np.random.shuffle(train_data)  # Shuffle training data for stochastic updates\n",
    "            \n",
    "            for user, item, rating in train_data:\n",
    "                # Predict rating\n",
    "                pred = np.dot(self.user_factors[user], self.item_factors[item])\n",
    "                error = rating - pred\n",
    "\n",
    "                # Update user and item latent factors\n",
    "                self.user_factors[user] += self.learning_rate * (error * self.item_factors[item] - self.reg_param * self.user_factors[user])\n",
    "                self.item_factors[item] += self.learning_rate * (error * self.user_factors[user] - self.reg_param * self.item_factors[item])\n",
    "\n",
    "            # Compute RMSE after each epoch\n",
    "            rmse = self.compute_rmse(df_train)\n",
    "            print(f\"Epoch {epoch + 1}/{self.n_epochs}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "    def compute_rmse(self, df):\n",
    "        \"\"\"\n",
    "        Compute RMSE on given dataset.\n",
    "        \"\"\"\n",
    "        errors = [(rating - np.dot(self.user_factors[user], self.item_factors[item])) ** 2 \n",
    "                  for user, item, rating in zip(df['user'], df['item'], df['rating'])]\n",
    "        return np.sqrt(np.mean(errors))\n",
    "\n",
    "    def predict(self, user, item):\n",
    "        \"\"\"\n",
    "        Predict a rating for a given user-item pair.\n",
    "        \"\"\"\n",
    "        return np.dot(self.user_factors[user], self.item_factors[item])\n",
    "\n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Reconstruct the full user-item rating matrix.\n",
    "        \"\"\"\n",
    "        return np.dot(self.user_factors, self.item_factors.T)\n",
    "\n",
    "\n",
    "# Example Usage with DataFrame\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulated user-item-rating data\n",
    "num_users = 100  # Total number of users\n",
    "num_items = 500 \n",
    "model = SGDMatrixFactorization(user_item_matrix.shape[0], user_item_matrix.shape[1], n_factors=1, learning_rate=0.01, reg_param=0.02, n_epochs=20)\n",
    "model.train(df)\n",
    "\n",
    "# Make a prediction for user 0 and item 10\n",
    "predicted_rating = model.predict(user=0, item=10)\n",
    "print(f\"Predicted rating for user 0 and item 10: {predicted_rating:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>evaluate</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.item_factors[:,0]\n",
    "model.user_factors[:,0]\n",
    "\n",
    "\n",
    "reconstructed_ratings = np.dot(model.user_factors, model.item_factors.T)\n",
    "actual_ratings = user_item_matrix[user_item_matrix > 0]\n",
    "predicted_ratings = reconstructed_ratings[user_item_matrix > 0]\n",
    "rmse = np.sqrt(mean_squared_error(user_item_matrix, reconstructed_ratings))\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ALS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class ALS:\n",
    "    def __init__(self, num_users, num_items, n_factors=10, reg_param=0.02, n_epochs=20):\n",
    "        \"\"\"\n",
    "        Alternating Least Squares (ALS) Matrix Factorization for Collaborative Filtering\n",
    "        :param num_users: Number of unique users\n",
    "        :param num_items: Number of unique items\n",
    "        :param n_factors: Number of latent factors\n",
    "        :param reg_param: Regularization parameter to avoid overfitting\n",
    "        :param n_epochs: Number of epochs to run ALS\n",
    "        \"\"\"\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.n_factors = n_factors\n",
    "        self.reg_param = reg_param\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        # Initialize the user and item matrices with small random values\n",
    "        self.user_factors = np.random.normal(scale=0.1, size=(num_users, n_factors))\n",
    "        self.item_factors = np.random.normal(scale=0.1, size=(num_items, n_factors))\n",
    "\n",
    "    def train(self, df):\n",
    "        \"\"\"\n",
    "        Train the ALS model on a DataFrame containing user-item-rating interactions.\n",
    "        :param df: Pandas DataFrame containing columns ['user_id', 'item_id', 'rating']\n",
    "        \"\"\"\n",
    "        # Convert DataFrame to a list of tuples (user_id, item_id, rating)\n",
    "        train_data = list(zip(df['user_id'], df['item_id'], df['rating']))\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Alternating updates\n",
    "            for user, item, rating in train_data:\n",
    "                # Update user matrix\n",
    "                self.user_factors[user] = self.update_user_factors(user, item, rating)\n",
    "                # Update item matrix\n",
    "                self.item_factors[item] = self.update_item_factors(user, item, rating)\n",
    "\n",
    "            # Optionally compute RMSE or other evaluation metrics\n",
    "            rmse = self.compute_rmse(df)\n",
    "            print(f\"Epoch {epoch+1}/{self.n_epochs}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "    def update_user_factors(self, user, item, rating):\n",
    "        \"\"\"\n",
    "        Update user latent factors for a specific user-item-rating triplet\n",
    "        :param user: user index\n",
    "        :param item: item index\n",
    "        :param rating: actual rating\n",
    "        \"\"\"\n",
    "        item_factors = self.item_factors[item]\n",
    "        predicted_rating = np.dot(self.user_factors[user], item_factors)\n",
    "        error = rating - predicted_rating\n",
    "\n",
    "        # Regularized update rule\n",
    "        self.user_factors[user] += self.reg_param * error * item_factors\n",
    "        return self.user_factors[user]\n",
    "\n",
    "    def update_item_factors(self, user, item, rating):\n",
    "        \"\"\"\n",
    "        Update item latent factors for a specific user-item-rating triplet\n",
    "        :param user: user index\n",
    "        :param item: item index\n",
    "        :param rating: actual rating\n",
    "        \"\"\"\n",
    "        user_factors = self.user_factors[user]\n",
    "        predicted_rating = np.dot(user_factors, self.item_factors[item])\n",
    "        error = rating - predicted_rating\n",
    "\n",
    "        # Regularized update rule\n",
    "        self.item_factors[item] += self.reg_param * error * user_factors\n",
    "        return self.item_factors[item]\n",
    "\n",
    "    def compute_rmse(self, df):\n",
    "        \"\"\"\n",
    "        Compute RMSE on the given dataset.\n",
    "        :param df: DataFrame containing ['user_id', 'item_id', 'rating']\n",
    "        \"\"\"\n",
    "        errors = [(rating - np.dot(self.user_factors[user], self.item_factors[item])) ** 2 \n",
    "                  for user, item, rating in zip(df['user_id'], df['item_id'], df['rating'])]\n",
    "        return np.sqrt(np.mean(errors))\n",
    "\n",
    "    def predict(self, user, item):\n",
    "        \"\"\"\n",
    "        Predict a rating for a given user-item pair.\n",
    "        :param user: user index\n",
    "        :param item: item index\n",
    "        \"\"\"\n",
    "        return np.dot(self.user_factors[user], self.item_factors[item])\n",
    "\n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Reconstruct the full user-item rating matrix.\n",
    "        \"\"\"\n",
    "        return np.dot(self.user_factors, self.item_factors.T)\n",
    "\n",
    "# Example usage with a DataFrame\n",
    "np.random.seed(42)\n",
    "\n",
    "# Example DataFrame\n",
    "num_users = user_item_matrix.shape[0]  # Total number of users\n",
    "num_items = user_item_matrix.shape[1]  # Total number of items\n",
    "\n",
    "ratings_dict = {\n",
    "    \"user\": numerical_data['origname_encoded'].values,\n",
    "    \"item\": numerical_data['latlon_encoded'].values,\n",
    "    \"rating\": numerical_data['speed_m_s'].values\n",
    "}\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame({\n",
    "    'user_id': ratings_dict['user'],#np.random.randint(0, num_users, 10000),\n",
    "    'item_id': ratings_dict['item'],#np.random.randint(0, num_items, 10000),\n",
    "    'rating': ratings_dict['rating']#np.random.randint(1, 6, 10000)  # Ratings between 1 and 5\n",
    "})\n",
    "\n",
    "# Train the ALS model\n",
    "model_als = ALS(num_users, num_items, n_factors=1, reg_param=0.02, n_epochs=20)\n",
    "model_als.train(df_train)\n",
    "\n",
    "# Make a prediction for user 0 and item 10\n",
    "predicted_rating = model_als.predict(user=0, item=10)\n",
    "print(f\"Predicted rating for user 0 and item 10: {predicted_rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_als.item_factors[:,0]\n",
    "model_als.user_factors[:,0]\n",
    "\n",
    "\n",
    "reconstructed_ratings = np.dot(model_als.user_factors, model_als.item_factors.T)\n",
    "actual_ratings = user_item_matrix[user_item_matrix > 0]\n",
    "predicted_ratings = reconstructed_ratings[user_item_matrix > 0]\n",
    "rmse = np.sqrt(mean_squared_error(user_item_matrix, reconstructed_ratings))\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> make tiiff</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.item_factors[:0].shape)\n",
    "user_item_matrix.columns\n",
    "Vt[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vals=model.item_factors[:,0]\n",
    "#output_file = \"grid_mapALS.tif\"\n",
    "\n",
    "vals=Vt[0,:]\n",
    "output_file = \"grid_mapSVD.tif\"\n",
    "# vals=H_nmf[:,0]\n",
    "# output_file = \"grid_mapNMF.tif\"\n",
    "\n",
    "# vals=item_factors[:,0]\n",
    "# output_file = \"grid_mapTrunc_SVD.tif\"\n",
    "\n",
    "# vals=i_factors[:,0]\n",
    "# output_file = \"grid_mapTrunc_fastica.tif\"\n",
    "\n",
    "# vals=model.item_factors[:,0]\n",
    "# output_file = \"grid_mapSGD.tif\"\n",
    "\n",
    "# vals=model_als.item_factors[:,0]\n",
    "# output_file = \"grid_mapALS.tif\"\n",
    "\n",
    "frameForTiff =pd.DataFrame(data={\n",
    "    'item_code':user_item_matrix.columns,\n",
    " #   'value':item_factors[:,0]*1000,\n",
    "# 'value':H.T[:,0]*1000,\n",
    "'value':vals,\n",
    "    'latlon':[latlon_dict.get(x) for x in user_item_matrix.columns ]\n",
    "})\n",
    "frameForTiff[['lat1', 'lon1']] = frameForTiff['latlon'].str.split('_', expand=True).astype(float)\n",
    "\n",
    "\n",
    "frameForTiff[\"lon\"] = [lon_dict.get(x) for x in user_item_matrix.columns ]\n",
    "frameForTiff[\"lat\"] = [lat_dict.get(x) for x in user_item_matrix.columns ]\n",
    "frameForTiff[\"N1\"] = [N1_dict.get(x) for x in user_item_matrix.columns ]\n",
    "frameForTiff[\"E1\"] = [E1_dict.get(x) for x in user_item_matrix.columns ]\n",
    "frameForTiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "# ✅ 1. Load Data (Example DataFrame)\n",
    "# Replace with actual DataFrame loading\n",
    "df = pd.DataFrame({\n",
    "    'lon': frameForTiff['lat1'] ,  # X-coordinates in EPSG:3765\n",
    "    'lat': frameForTiff['lon1'] ,  # Y-coordinates in EPSG:3765\n",
    "    'value': frameForTiff['value']  # Example values with NaN\n",
    "})\n",
    "\n",
    "# ✅ 2. Define Grid Extent & Resolution\n",
    "resolution = 100  # Set grid cell size in meters (adjust as needed)\n",
    "\n",
    "# Compute grid boundaries\n",
    "min_x, max_x = df['lon'].min(), df['lon'].max()\n",
    "min_y, max_y = df['lat'].min(), df['lat'].max()\n",
    "\n",
    "# Compute grid size\n",
    "cols = int((max_x - min_x) / resolution) + 1\n",
    "rows = int((max_y - min_y) / resolution) + 1\n",
    "\n",
    "# ✅ 3. Create Empty Grid with NaN\n",
    "grid = np.full((rows, cols), np.nan, dtype=np.float32)\n",
    "\n",
    "# ✅ 4. Map DataFrame Values to Grid\n",
    "for _, row in df.iterrows():\n",
    "    col_idx = int((row['lon'] - min_x) / resolution)\n",
    "    row_idx = int((max_y - row['lat']) / resolution)  # Inverted y-axis\n",
    "    grid[row_idx, col_idx] = row['value']\n",
    "\n",
    "# ✅ 5. Define GeoTransform\n",
    "transform = from_origin(min_x, max_y, resolution, resolution)\n",
    "\n",
    "# ✅ 6. Save as GeoTIFF\n",
    "\n",
    "with rasterio.open(\n",
    "    output_file,\n",
    "    \"w\",\n",
    "    driver=\"GTiff\",\n",
    "    height=rows,\n",
    "    width=cols,\n",
    "    count=1,\n",
    "    dtype=np.float32,\n",
    "    crs=\"EPSG:3765\",\n",
    "    transform=transform,\n",
    "    nodata=np.nan\n",
    ") as dst:\n",
    "    dst.write(grid, 1)\n",
    "\n",
    "print(f\"GeoTIFF map saved as '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.T[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "# Sample DataFrame with 'value' column\n",
    "\n",
    "\n",
    "# Split 'latlon' into 'lat' and 'lon'\n",
    " \n",
    "frameForTiff[['lat', 'lon']] = frameForTiff['latlon'].str.split('_', expand=True).astype(float)\n",
    " \n",
    "# Define extent\n",
    "min_lon, max_lon = frameForTiff['lon'].min(), frameForTiff['lon'].max()\n",
    "min_lat, max_lat = frameForTiff['lat'].min(), frameForTiff['lat'].max()\n",
    "\n",
    "# Determine grid resolution (smallest step size)\n",
    "lon_step = np.min(np.diff(np.sort(frameForTiff['lon'].unique())))\n",
    "lat_step = np.min(np.diff(np.sort(frameForTiff['lat'].unique())))\n",
    "\n",
    "# Define grid size\n",
    "cols = int((max_lon - min_lon) / lon_step) + 1\n",
    "rows = int((max_lat - min_lat) / lat_step) + 1\n",
    "\n",
    "# Create an empty raster grid filled with NaN\n",
    "grid = np.full((rows, cols), np.nan, dtype=np.float32)\n",
    "\n",
    "# Fill the grid with 'value' from the DataFrame\n",
    "for _, row in frameForTiff.iterrows():\n",
    "    col_idx = int((row['lon'] - min_lon) / lon_step)\n",
    "    row_idx = int((max_lat - row['lat']) / lat_step)  # Invert Y-axis for raster order\n",
    "    grid[row_idx, col_idx] = row['value']\n",
    "\n",
    "# Define transformation\n",
    "transform = from_origin(min_lon, max_lat, lon_step, lat_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "# Sample DataFrame with 'value' column\n",
    "\n",
    "\n",
    "# Split 'latlon' into 'lat' and 'lon'\n",
    "\n",
    " \n",
    "# Define extent\n",
    "min_N1, max_N1 = frameForTiff['N1'].min(), frameForTiff['N1'].max()\n",
    "min_E1, max_E1 = frameForTiff['E1'].min(), frameForTiff['E1'].max()\n",
    "min_lon, max_lon = frameForTiff['lon'].min(), frameForTiff['lon'].max()\n",
    "min_lat, max_lat = frameForTiff['lat'].min(), frameForTiff['lat'].max()\n",
    "\n",
    "# Determine grid resolution (smallest step size)\n",
    "N1_step = np.min(np.diff(np.sort(frameForTiff['N1'].unique())))\n",
    "E1_step = np.min(np.diff(np.sort(frameForTiff['E1'].unique())))\n",
    "print(N1_step)\n",
    "# Define grid size\n",
    "cols = int((max_N1 - min_N1) / N1_step) + 1\n",
    "rows = int((max_E1 - min_E1) / E1_step) + 1\n",
    "\n",
    "# Create an empty raster grid filled with NaN\n",
    "grid = np.full((rows, cols), np.nan, dtype=np.float32)\n",
    "\n",
    "# Fill the grid with 'value' from the DataFrame\n",
    "for _, row in frameForTiff.iterrows():\n",
    "    col_idx = int((row['N1'] - min_N1) / N1_step)\n",
    "    row_idx = int((max_E1 - row['E1']) / E1_step)  # Invert Y-axis for raster order\n",
    "    grid[row_idx, col_idx] = row['value']\n",
    "\n",
    "# Define transformation\n",
    "transform = from_origin(min_lon, max_lat, N1_step, E1_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(\n",
    "    \"grid_map4.tif\",\n",
    "    \"w\",\n",
    "    driver=\"GTiff\",\n",
    "    height=rows,\n",
    "    width=cols,\n",
    "    count=1,\n",
    "    dtype=np.float32,\n",
    "    crs= \"epsg:3765\",  # Change if needed\"epsg:3765\"\n",
    "    transform=transform\n",
    ") as dst:\n",
    "    dst.write(grid, 1)\n",
    "\n",
    "print(\"GeoTIFF map saved as 'grid_map.tif'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python.exe -m pip install --upgrade pip\n",
    "#pip install surprise\n",
    "%pip install --upgrade pip setuptools wheel cython numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data=pd.read_csv('numerical_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NMF sklearn</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import mean_squared_error\n",
    "ratings_dict = {\n",
    "    \"user\": numerical_data['origname_encoded'].values,\n",
    "    \"item\": numerical_data['latlon_encoded'].values,\n",
    "    \"rating\": numerical_data['speed_m_s'].values\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(ratings_dict)\n",
    "print(df)\n",
    "user_item_matrix = df.pivot(index=\"user\", columns=\"item\", values=\"rating\").fillna(0)\n",
    "#print(user_item_matrix)\n",
    "\n",
    "nmf = NMF(n_components=1, init='random', random_state=42)\n",
    "\n",
    "# Factorize user-item matrix into W (Users) and H (Items)\n",
    "W = nmf.fit_transform(user_item_matrix)  # User features\n",
    "H = nmf.components_  # Item features\n",
    "\n",
    "#print(\"User Features Matrix (W):\\n\", W)\n",
    "#print(\"Item Features Matrix (H):\\n\", H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=1, init='random', random_state=42)\n",
    "\n",
    "# Factorize user-item matrix into W (Users) and H (Items)\n",
    "W = nmf.fit_transform(user_item_matrix)  # User features\n",
    "H = nmf.components_  # Item features\n",
    "H.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix['latent']=H.T\n",
    "user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from surprise.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "\n",
    "# Scale represents speed possibilities from 0m/s to 9m/s\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(numerical_data, reader)\n",
    "data.df\n",
    "from surprise import accuracy, Dataset, SVD, NMF, SVDpp,BaselineOnly,KNNBasic,SlopeOne,CoClustering\n",
    "from surprise.model_selection import KFold\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# define a cross-validation iterator\n",
    "#kf = KFold(n_splits=2,shuffle=True)\n",
    "\n",
    "#algo = SVD()\n",
    "algos={\n",
    "    'svd':SVD(),\n",
    "    'NMF':NMF(),\n",
    "    #,'SVDpp':SVDpp(),\n",
    "   # 'BaselineOnly':BaselineOnly(),\n",
    "    #'SlopeOne':SlopeOne(),\n",
    "    'CoClustering':CoClustering()\n",
    "}\n",
    "for  algo in algos:\n",
    "  print(algo)\n",
    "  #print(cross_validate(algos[algo], data, cv=5))\n",
    "  cross_validate(algos[algo], data, measures=[\"RMSE\", \"MAE\"], cv=3, verbose=True)\n",
    "  print()\n",
    "  print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install sklearn\n",
    "#%pip install git+https://github.com/NicolasHug/Surprise.git\n",
    "#%pip install scikit-surprise\n",
    "###%pip install --upgrade pip setuptools wheel\n",
    "\n",
    "!pip install scikit-surprise --no-build-isolation\n",
    "##%pip install cython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import mean_squared_error\n",
    "ratings_dict = {\n",
    "    \"user\": numerical_data['origname_encoded'].values,\n",
    "    \"item\": numerical_data['latlon_encoded'].values,\n",
    "    \"rating\": numerical_data['speed_m_s'].values\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(ratings_dict)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = df.pivot(index=\"user\", columns=\"item\", values=\"rating\").fillna(0)\n",
    "#print(user_item_matrix)\n",
    "\n",
    "nmf = NMF(n_components=2, init='random', random_state=42)\n",
    "\n",
    "# Factorize user-item matrix into W (Users) and H (Items)\n",
    "W = nmf.fit_transform(user_item_matrix)  # User features\n",
    "H = nmf.components_  # Item features\n",
    "\n",
    "#print(\"User Features Matrix (W):\\n\", W)\n",
    "#print(\"Item Features Matrix (H):\\n\", H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Number of clusters\n",
    "num_clusters = 5\n",
    "item_features_df = pd.DataFrame(H.T, columns=[\"Feature 1\", \"Feature 2\"])\n",
    "\n",
    "# Perform clustering on item features\n",
    "#spectral_clustering = SpectralClustering(n_clusters=num_clusters, affinity=\"nearest_neighbors\", random_state=42)\n",
    "#item_clusters = spectral_clustering.fit_predict(H.T)\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "item_clusters = kmeans.fit_predict(H.T)\n",
    "# Assign clusters to items\n",
    "item_features_df[\"Cluster\"] = item_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot item clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=item_features_df[\"Feature 1\"],\n",
    "    y=item_features_df[\"Feature 2\"],\n",
    "    hue=item_features_df[\"Cluster\"],\n",
    "    palette=\"viridis\",\n",
    "    s=100\n",
    ")\n",
    "\n",
    "# Add item labels\n",
    "#\n",
    "plt.xlabel(\"Latent Feature 1\")\n",
    "plt.ylabel(\"Latent Feature 2\")\n",
    "plt.title(\"Item Clusters in Latent Feature Space (NMF)\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Number of clusters\n",
    "num_clusters = 5\n",
    "euser_features_df = pd.DataFrame(W, columns=[\"Feature 1\", \"Feature 2\"])\n",
    "\n",
    "# Perform clustering on item features\n",
    "#spectral_clustering = SpectralClustering(n_clusters=num_clusters, affinity=\"nearest_neighbors\", random_state=42)\n",
    "#item_clusters = spectral_clustering.fit_predict(H.T)\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "item_clusters = kmeans.fit_predict(W)\n",
    "# Assign clusters to items\n",
    "euser_features_df[\"Cluster\"] = item_clusters\n",
    "euser_features_df[\"Cluster\"] = item_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot item clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=euser_features_df[\"Feature 1\"],\n",
    "    y=euser_features_df[\"Feature 2\"],\n",
    "    hue=euser_features_df[\"Cluster\"],\n",
    "    palette=\"viridis\",\n",
    "    s=100\n",
    ")\n",
    "\n",
    "# Add item labels\n",
    "#\n",
    "plt.xlabel(\"Latent Feature 1\")\n",
    "plt.ylabel(\"Latent Feature 2\")\n",
    "plt.title(\"Item Clusters in Latent Feature Space (NMF)\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_matrix = np.dot(W, H)\n",
    "print(\"Predicted Ratings Matrix:\\n\", predicted_matrix)\n",
    "# Convert to DataFrame for easy comparison\n",
    "pred_df = pd.DataFrame(predicted_matrix, index=user_item_matrix.index, columns=user_item_matrix.columns)\n",
    "\n",
    "# Calculate RMSE only for non-zero ratings\n",
    "true_ratings = user_item_matrix.to_numpy().flatten()\n",
    "pred_ratings = pred_df.to_numpy().flatten()\n",
    "mask = true_ratings > 0  # Only compare actual ratings\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(true_ratings[mask], pred_ratings[mask]))\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow numpy pandas scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c conda-forge scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
